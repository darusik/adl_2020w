{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXTURE_SIZE = 400\n",
    "\n",
    "_CHEEK_L = 0.35\n",
    "_CHEEK_M = 0.5\n",
    "_CHEEK_R = 0.65\n",
    "_CHEEK_T = 0.3125\n",
    "_CHEEK_B = 0.4125\n",
    "\n",
    "_FACE_TIGHT_BOUNDS_L = 0.25\n",
    "_FACE_TIGHT_BOUNDS_R = 0.75\n",
    "_FACE_TIGHT_BOUNDS_T = 0.125\n",
    "_FACE_TIGHT_BOUNDS_B = 0.625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import uv\n",
    "\n",
    "def get_fixed_process_uv(original_func):\n",
    "    def fixed_process_uv(uv_coords, *args, **kwargs):\n",
    "        return original_func(uv_coords.copy(), *args, **kwargs)\n",
    "    return fixed_process_uv\n",
    "\n",
    "uv.process_uv = get_fixed_process_uv(uv.process_uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import yaml\n",
    "\n",
    "from FaceBoxes import FaceBoxes\n",
    "from TDDFA import TDDFA\n",
    "from utils.render import render, render_app\n",
    "from utils.functions import cv_draw_landmark\n",
    "from utils.depth import depth\n",
    "from utils.pncc import pncc\n",
    "from utils.uv import uv_tex, get_colors, process_uv, g_uv_coords\n",
    "from utils.pose import viz_pose\n",
    "\n",
    "from utils.tddfa_util import _to_ctype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    CONFIG = 'configs/mb1_120x120.yml'\n",
    "    MODE = 'cpu'\n",
    "    ONNX = True\n",
    "\n",
    "    cfg = yaml.load(open(CONFIG), Loader=yaml.SafeLoader)\n",
    "\n",
    "    # Init FaceBoxes and TDDFA, recommend using onnx flag\n",
    "    if ONNX:\n",
    "        import os\n",
    "        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "        os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "        from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n",
    "        from TDDFA_ONNX import TDDFA_ONNX\n",
    "\n",
    "        face_boxes = FaceBoxes_ONNX()\n",
    "        tddfa = TDDFA_ONNX(**cfg)\n",
    "    else:\n",
    "        gpu_mode = MODE == 'gpu'\n",
    "        tddfa = TDDFA(gpu_mode=gpu_mode, **cfg)\n",
    "        face_boxes = FaceBoxes()\n",
    "        \n",
    "    return face_boxes, tddfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertices_predictor(face_boxes, tddfa):\n",
    "    dense_flag = True\n",
    "\n",
    "    def predictor(frame_bgr):\n",
    "        boxes = face_boxes(frame_bgr)\n",
    "        boxes = [boxes[0]]\n",
    "        param_lst, roi_box_lst = tddfa(frame_bgr, boxes)\n",
    "        ver = tddfa.recon_vers(param_lst, roi_box_lst, dense_flag=dense_flag)[0]\n",
    "\n",
    "        # refine\n",
    "        param_lst, roi_box_lst = tddfa(frame_bgr, [ver], crop_policy='landmark')\n",
    "        ver = tddfa.recon_vers(param_lst, roi_box_lst, dense_flag=dense_flag)[0]\n",
    "        \n",
    "        return ver\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr_to_rgb(image):\n",
    "    if image.ndim == 3:\n",
    "        image = image.copy()\n",
    "        image[..., :3] = image[..., 2::-1]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from IPython.display import display\n",
    "\n",
    "def imshow(bgr):\n",
    "    display(PIL.Image.fromarray(bgr_to_rgb(bgr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def pltshow(bgr):\n",
    "    plt.imshow(bgr_to_rgb(bgr))\n",
    "    plt.gca().set_xticks([])\n",
    "    plt.gca().set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "def show_channelwise(array):\n",
    "    if isinstance(array, list) or isinstance(array, tuple):\n",
    "        array = np.stack(array, axis=-1)\n",
    "        \n",
    "    n = array.shape[-1]\n",
    "    for c in range(n):\n",
    "        plt.subplot(1, n, c + 1)\n",
    "        plt.imshow(bgr_to_rgb(array[..., c]))\n",
    "        plt.gca().set_xticks([])\n",
    "        plt.gca().set_yticks([])\n",
    "    plt.gcf().set_size_inches(6 * n, 6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uv_coords(texture_size):\n",
    "    assert texture_size > 0\n",
    "    uvs = process_uv(g_uv_coords.copy(), uv_h=texture_size, uv_w=texture_size)\n",
    "    return np.clip(np.round(uvs).astype(int), 0, texture_size - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_uv2ver(uv_coords, texture_size):\n",
    "    assert texture_size > 0\n",
    "    uv2ver_sparse = np.full((texture_size, texture_size, 3), fill_value=np.nan, dtype=np.float32)\n",
    "    uv2ver_sparse[uv_coords[:, 1], uv_coords[:, 0]] = ver.T\n",
    "    return uv2ver_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "# https://stackoverflow.com/questions/20516762/extrapolate-with-linearndinterpolator\n",
    "class LinearNDInterpolatorExt(object):\n",
    "    def __init__(self, points, values):\n",
    "        self.funcinterp = interpolate.LinearNDInterpolator(points, values)\n",
    "        self.funcnearest = interpolate.NearestNDInterpolator(points, values)\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        z = self.funcinterp(*args)\n",
    "        chk = np.isnan(z)\n",
    "        if chk.any():\n",
    "            return np.where(chk, self.funcnearest(*args), z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/21690608/numpy-inpaint-nans-interpolate-and-extrapolate\n",
    "def inpaint(image, mask, interpolator_cls):\n",
    "    assert image.shape == mask.shape and image.ndim == 3\n",
    "    image = np.where(mask, 0, image)\n",
    "    \n",
    "    nd = np.reshape(list(np.ndindex(image.shape[:2])), image.shape[:2] + (2, ))\n",
    "\n",
    "    for c in range(image.shape[-1]):\n",
    "        image_c = image[..., c]\n",
    "        mask_c = mask[..., c]\n",
    "\n",
    "        valid_mask = np.logical_not(mask_c) * (~np.isnan(image_c))\n",
    "        coords = np.array(np.nonzero(valid_mask)).T\n",
    "        values = image_c[valid_mask]\n",
    "\n",
    "        it = interpolator_cls(coords, values)\n",
    "        nd_masked = nd[np.where(mask_c)].reshape(-1, 2)\n",
    "        image_c[np.where(mask_c)] = it(nd_masked)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_indicator(uv2ver, k=17):\n",
    "    assert k >= 0\n",
    "    face_indicator = (~np.isnan(uv2ver)).max(axis=-1).astype(np.uint8)\n",
    "    \n",
    "    if k > 0:\n",
    "        face_indicator = cv2.erode(\n",
    "            np.pad(face_indicator, pad_width=1, mode='constant', constant_values=0),\n",
    "            np.ones((17, 17), np.uint8),\n",
    "        )[1:-1, 1:-1]\n",
    "    \n",
    "    return face_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.draw import line_aa\n",
    "\n",
    "def draw_interpolated_line(src, dst, dst_w, pt1, pt2, blend_w_src=0.0):\n",
    "    assert src.ndim == dst.ndim == dst_w.ndim == 3\n",
    "    assert src.shape == dst.shape == dst_w.shape\n",
    "\n",
    "    rr, cc, alpha = line_aa(*pt1[::-1], *pt2[::-1])\n",
    "\n",
    "    tt = np.linspace(0.0, 1.0, num=len(alpha))\n",
    "    \n",
    "    v1 = src[pt1[1], pt1[0]]\n",
    "    v2 = src[pt2[1], pt2[0]]\n",
    "    interp = (v1[None,] * (1.0 - tt[..., None]) + v2[None,] * tt[..., None])\n",
    "    \n",
    "    assert 0.0 <= blend_w_src <= 1.0\n",
    "    if 0.0 < blend_w_src:\n",
    "        interp = interp * (1.0 - blend_w_src) + src[rr, cc] * blend_w_src\n",
    "    \n",
    "    dst[rr, cc] += interp * alpha[..., None]\n",
    "    dst_w[rr, cc] += alpha[..., None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "\n",
    "def get_almost_convex_sparse_uv2ver(ver, uv2ver, face_indicator):\n",
    "\n",
    "    def is_near_nose_tip(pt):\n",
    "        return 0.45 <= (float(pt[0]) / uv2ver.shape[1]) <= 0.55 and \\\n",
    "               0.32 <= (float(pt[1]) / uv2ver.shape[0]) <= 0.42\n",
    "\n",
    "    def is_higher_than_nose(pt):\n",
    "        return (float(pt[1]) / uv2ver.shape[0]) <= 0.37\n",
    "\n",
    "    def is_nasal_bridge(pt1, pt2):\n",
    "        return (is_near_nose_tip(pt1) and is_higher_than_nose(pt2)) or \\\n",
    "               (is_near_nose_tip(pt2) and is_higher_than_nose(pt1))\n",
    "    \n",
    "    hull = ConvexHull(ver.T)\n",
    "\n",
    "    convex_map_acc = np.zeros(uv2ver.shape, dtype=np.float32)\n",
    "    convex_map_w = np.zeros(uv2ver.shape, dtype=np.float32)\n",
    "\n",
    "    for simplex in hull.simplices:\n",
    "        pts = [tuple(uv_coords[s, :2]) for s in simplex]\n",
    "        if all(face_indicator[pt[1], pt[0]] == 0 for pt in pts): continue\n",
    "\n",
    "        for st, en in [(pts[0], pts[1]), (pts[1], pts[2]), (pts[2], pts[0])]:\n",
    "            blend_w_src = 0.75 if is_nasal_bridge(st, en) else 0.0\n",
    "            draw_interpolated_line(uv2ver, convex_map_acc, convex_map_w, st, en, blend_w_src)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        convex_map_sparse = convex_map_acc / convex_map_w\n",
    "    assert np.isnan(convex_map_sparse).any()\n",
    "    \n",
    "    return convex_map_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def blur_multichannel(array, *args, **kwargs):\n",
    "    assert array.ndim == 3\n",
    "    blurred = array.copy()\n",
    "    for c in range(array.shape[-1]):\n",
    "        blurred[..., c] = gaussian_filter(array[..., c], *args, **kwargs)\n",
    "    return blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blur_weights(shape):\n",
    "    y_levels = np.mgrid[0:shape[0], 0:shape[1]][0].astype(np.float32) / shape[0]\n",
    "    return np.clip(1.0 - (np.clip(y_levels, 0.37, 0.48) - 0.37) / 0.11, 0.2, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import warp\n",
    "from random import uniform\n",
    "\n",
    "class FacecoverTextureWarper:\n",
    "    @staticmethod\n",
    "    def _get_facecover2tex(facecover_size, texture_side):\n",
    "        assert len(facecover_size) == 2 and all(v > 0 for v in facecover_size)\n",
    "        assert texture_side > 0\n",
    "\n",
    "        # X/Y coordinates\n",
    "        ear_top_y = uniform(0.35, 0.425)\n",
    "        facecover_mid_top_y = uniform(0.375, 0.420)\n",
    "        facecover_top_y = uniform(0.26, 0.38)\n",
    "        \n",
    "        ear_bottom_y = uniform(0.615, 0.650)\n",
    "        facecover_mid_bottom_y = uniform(0.66, 0.71)\n",
    "        facecover_bottom_y = uniform(0.7, 0.8)\n",
    "        \n",
    "        facecover_near_top_y = facecover_top_y * 0.8 + facecover_bottom_y * 0.2\n",
    "        facecover_near_bottom_y = facecover_bottom_y * 0.8 + facecover_top_y * 0.2\n",
    "        \n",
    "        facecover_half_width = uniform(0.18, 0.32)\n",
    "        \n",
    "        # Anchor points\n",
    "        ear_lt = (0.05, ear_top_y)\n",
    "        ear_lb = (0.10, ear_bottom_y)\n",
    "        ear_rt = (0.95, ear_top_y)\n",
    "        ear_rb = (0.90, ear_bottom_y)\n",
    "        \n",
    "        facecover_lmt = (0.5 - facecover_half_width, facecover_mid_top_y)\n",
    "        facecover_lmb = (0.5 - facecover_half_width, facecover_mid_bottom_y)\n",
    "        facecover_rmt = (0.5 + facecover_half_width, facecover_mid_top_y)\n",
    "        facecover_rmb = (0.5 + facecover_half_width, facecover_mid_bottom_y)\n",
    "        \n",
    "        facecover_top = (0.5, facecover_top_y)\n",
    "        facecover_near_top = (0.5, facecover_near_top_y)\n",
    "        facecover_near_bottom = (0.5, facecover_near_bottom_y)\n",
    "        facecover_bottom = (0.5, facecover_bottom_y)\n",
    "        \n",
    "        # Mapping from facecover-space to texture space\n",
    "        \n",
    "        facecover_left2tex = np.array([\n",
    "            (0.0 , 0.0) + ear_lt,\n",
    "            (0.25, 0.0) + facecover_lmt,\n",
    "            (0.25, 1.0) + facecover_lmb,\n",
    "            (0.0 , 1.0) + ear_lb,\n",
    "        ])\n",
    "        facecover_center2tex = np.array([\n",
    "            (0.25, 0.0) + facecover_lmt,\n",
    "            (0.5 , 0.0) + facecover_top,\n",
    "            (0.5 , 0.2) + facecover_near_top,\n",
    "            (0.75, 0.0) + facecover_rmt,\n",
    "            (0.75, 1.0) + facecover_rmb,\n",
    "            (0.5 , 0.8) + facecover_near_bottom,\n",
    "            (0.5 , 1.0) + facecover_bottom,\n",
    "            (0.25, 1.0) + facecover_lmb,\n",
    "        ])\n",
    "        facecover_right2tex = np.array([\n",
    "            (1.0 , 0.0) + ear_rt,\n",
    "            (0.75, 0.0) + facecover_rmt,\n",
    "            (0.75, 1.0) + facecover_rmb,\n",
    "            (1.0 , 1.0) + ear_rb,\n",
    "        ])\n",
    "        \n",
    "        facecover2tex = [facecover_left2tex, facecover_center2tex, facecover_right2tex]\n",
    "        \n",
    "        for part2tex in facecover2tex:\n",
    "            part2tex[:, :2] *= [facecover_size]\n",
    "            part2tex[:, 2:] *= texture_side\n",
    "\n",
    "        return facecover2tex\n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_float_image(image):\n",
    "        if image.dtype == np.uint8:\n",
    "            return image.astype(np.float32) / 255.0\n",
    "        else:\n",
    "            return image.copy()\n",
    "        \n",
    "    def __call__(self, facecover_uv_texture, texture_side):\n",
    "        facecover_uv_texture = self._to_float_image(facecover_uv_texture)\n",
    "            \n",
    "        image = np.zeros((texture_side, texture_side, 4), dtype=np.float32)\n",
    "        \n",
    "        for part2tex in self._get_facecover2tex(facecover_uv_texture.shape[:2][::-1], texture_side):\n",
    "            it = interpolate.CloughTocher2DInterpolator(part2tex[:, 2:], part2tex[:, :2])\n",
    "            warped = warp(facecover_uv_texture, it, output_shape=(texture_side, texture_side, 4),\n",
    "                          order=3, mode='constant', cval=0.0)\n",
    "            # Alpha-blending\n",
    "            image = warped * warped[..., -1:] + image * (1.0 - warped[..., -1:])\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mesh(uv2ver, face_indicator, num_rects_h, num_rects_w):\n",
    "    assert num_rects_h > 0 and num_rects_w > 0\n",
    "\n",
    "    face_vertice_to_idx = {}\n",
    "    face_tri = []\n",
    "\n",
    "    face_v_slices = np.linspace(0, num_rects_w - 1, num=num_rects_w)\n",
    "    face_h_slices = np.linspace(0, num_rects_h - 1, num=num_rects_h)\n",
    "\n",
    "    for face_vt, face_vb in zip(face_v_slices[:-1], face_v_slices[1:]):\n",
    "        for face_vl, face_vr in zip(face_h_slices[:-1], face_h_slices[1:]):\n",
    "            lt = (face_vl, face_vt)\n",
    "            rt = (face_vr, face_vt)\n",
    "            lb = (face_vl, face_vb)\n",
    "            rb = (face_vr, face_vb)\n",
    "\n",
    "            for v in [lt, rt, lb, rb]:\n",
    "                v_idx = face_vertice_to_idx.get(v, len(face_vertice_to_idx))\n",
    "                face_vertice_to_idx[v] = v_idx\n",
    "\n",
    "            for vs in [(lt, rt, lb), (rb, lb, rt)]:\n",
    "                face_tri.append([face_vertice_to_idx[v] for v in vs])\n",
    "\n",
    "    verts_uv = np.zeros((len(face_vertice_to_idx), 2), dtype=int)\n",
    "    for v, idx in face_vertice_to_idx.items():\n",
    "        verts_uv[idx] = v\n",
    "\n",
    "    face_ver = np.array([uv2ver[v, u] for u, v in verts_uv], dtype=np.float32).T\n",
    "    ver_not_on_face = np.array([face_indicator[v, u] < 1 for u, v in verts_uv],\n",
    "                               dtype=face_indicator.dtype)\n",
    "    \n",
    "    face_tri = np.array(face_tri, dtype=np.int32)\n",
    "    tri_not_on_face = ver_not_on_face[face_tri].any(axis=-1)\n",
    "\n",
    "    face_tri = face_tri[np.where(~tri_not_on_face)[0]]\n",
    "\n",
    "    face_colors = np.array([[0, 0, 0] for u, v in verts_uv], dtype=np.float32)\n",
    "    face_alphas = np.array([1.0 for u, v in verts_uv], dtype=np.float32)\n",
    "    return dict(ver=face_ver, tri=face_tri, colors=face_colors, alphas=face_alphas, uv=verts_uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_colors(mesh, facecover):\n",
    "    mesh['colors'] = np.array([facecover[v, u][:3] for u, v in mesh['uv']], dtype=np.float32)\n",
    "    mesh['alphas'] = np.array([facecover[v, u][-1] for u, v in mesh['uv']], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visible_skin(uv2ver):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        visible_skin = (np.gradient(uv2ver[..., 0], axis=1) > 0.666).astype(np.float32)\n",
    "    visible_skin[np.where(visible_skin < 1)] = np.nan\n",
    "    return visible_skin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_both_sides_visible(visible_skin, threshold=0.1):\n",
    "    left_sum = np.nansum(visible_skin[:, :visible_skin.shape[1] // 2])\n",
    "    right_sum = np.nansum(visible_skin[:, visible_skin.shape[1] // 2:])\n",
    "    return min(left_sum, right_sum) / max(left_sum, right_sum) > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ratio(left, right):\n",
    "    return (-left / max(right, 1e-7) + 1) if left > right else (right / max(left, 1e-7) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cheeks_lightness_ratio(texture, visible_skin):\n",
    "    H, W = texture.shape[:2]\n",
    "    lightness = cv2.cvtColor(texture, cv2.COLOR_BGR2HLS)[..., 1].astype(np.float32) / 255.0\n",
    "\n",
    "    visible_lightness = np.where(np.isnan(visible_skin), np.nan, lightness)\n",
    "    left_cheek = visible_lightness[int(H * _CHEEK_T):int(H * _CHEEK_B), int(W * _CHEEK_L):int(W * _CHEEK_M)]\n",
    "    right_cheek = visible_lightness[int(H * _CHEEK_T):int(H * _CHEEK_B), int(W * _CHEEK_M):int(W * _CHEEK_R)]\n",
    "    return calculate_ratio(np.nanmean(left_cheek), np.nanmean(right_cheek))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_light_x_position(target_ratio, verts_uv, ver, tri, texture, visible_skin, \n",
    "                          render_app, background_shape):\n",
    "    if not is_both_sides_visible(visible_skin) or np.isnan(target_ratio):\n",
    "        return 0\n",
    "    \n",
    "    original_light_pos = render_app.light_pos\n",
    "    \n",
    "    H, W = texture.shape[:2]\n",
    "\n",
    "    left_cheek_ver_indices = np.array([\n",
    "        i for i, (u, v, *_) in enumerate(verts_uv)\n",
    "        if _CHEEK_L <= (float(u) / W) <= _CHEEK_M and _CHEEK_T <= (float(v) / H) <= _CHEEK_B\n",
    "    ])\n",
    "    right_cheek_ver_indices = np.array([\n",
    "        i for i, (u, v, *_) in enumerate(verts_uv)\n",
    "        if _CHEEK_M <= (float(u) / W) <= _CHEEK_R and _CHEEK_T <= (float(v) / H) <= _CHEEK_B\n",
    "    ])\n",
    "    \n",
    "    tmp_img_placeholder = np.zeros(background_shape, dtype=np.uint8)\n",
    "\n",
    "    light_ratios = {}\n",
    "    \n",
    "    for x_light in range(-10, 10, 1):\n",
    "        render_app.light_pos = (x_light, 0, 5)\n",
    "        light_texture = np.full(ver.T.shape, fill_value=1.0, dtype=np.float32)\n",
    "        render_app(_to_ctype(ver.T), tri, tmp_img_placeholder, light_texture)\n",
    "\n",
    "        left_cheek_mean = light_texture[left_cheek_ver_indices].mean()\n",
    "        right_cheek_mean = light_texture[right_cheek_ver_indices].mean()\n",
    "\n",
    "        if left_cheek_mean > right_cheek_mean:\n",
    "            light_ratios[x_light] = -left_cheek_mean / max(right_cheek_mean, 1e-7) + 1\n",
    "        else:\n",
    "            light_ratios[x_light] = right_cheek_mean / max(left_cheek_mean, 1e-7) - 1\n",
    "\n",
    "    render_app.light_pos = original_light_pos\n",
    "    \n",
    "    best_light_x = min(light_ratios.items(), key=lambda kv: abs(kv[1] - target_ratio))[0]\n",
    "    # inc for positive, dec for negative, zero doesn't change\n",
    "    best_light_x = best_light_x + best_light_x / max(abs(best_light_x), 1)\n",
    "    return best_light_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/jrosebr1/color_transfer\n",
    "from color_transfer import color_transfer\n",
    "\n",
    "def facecover_color_transfer(facecover, texture):\n",
    "    assert facecover.ndim == 3 and facecover.shape[-1] == 4\n",
    "    H, W = texture.shape[:2]\n",
    "\n",
    "    solid = facecover[..., :3].copy()\n",
    "    alpha = facecover[..., -1]\n",
    "\n",
    "    for c in range(3):\n",
    "        solid[..., c][alpha < 0.1] = np.mean(solid[..., c][alpha > 0.1])\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('error')\n",
    "        try:\n",
    "            face_crop = texture[int(H * _FACE_TIGHT_BOUNDS_T):int(H * _FACE_TIGHT_BOUNDS_B),\n",
    "                                int(W * _FACE_TIGHT_BOUNDS_L):int(W * _FACE_TIGHT_BOUNDS_R)]\n",
    "            transferred = color_transfer(\n",
    "                face_crop, (solid * 255.0).astype(np.uint8), clip=False, preserve_paper=False\n",
    "            ).astype(np.float32) / 255.0\n",
    "            transferred = cv2.addWeighted(transferred, 0.25, solid, 0.75, 0.0)\n",
    "        except Warning:\n",
    "            transferred = solid.copy()\n",
    "\n",
    "    return np.concatenate([transferred, alpha[..., None]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lightning_params(target_ratio, light_x, facecover):\n",
    "    assert facecover.ndim == 3 and facecover.shape[-1] == 4 and facecover.dtype == np.float32\n",
    "\n",
    "    facecover_lightness = cv2.cvtColor(\n",
    "        (facecover[..., :3] * facecover[..., -1:] * 255.0).astype(np.uint8),\n",
    "        cv2.COLOR_BGR2HLS\n",
    "    )[..., 1] / 255.0\n",
    "\n",
    "    target_lightness = facecover_lightness.max()\n",
    "\n",
    "    # heuristic: ambient + direct = ambient + ambient * (|ratio| + 1) = target_lightness\n",
    "    ambient_w = target_lightness / (2.0 + abs(target_ratio))\n",
    "    # second multiplier is another neuristic\n",
    "    direct_w = (target_lightness - ambient_w) * np.power(abs(light_x) + 1, 0.125)\n",
    "    return ambient_w, direct_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_facecover(render_app, face_mesh, background, light_x, ambient_w, direct_w, \n",
    "                     return_intermediate=False):\n",
    "    original_light_pos = render_app.light_pos\n",
    "    original_light_intensity = (render_app.intensity_ambient,\n",
    "                                render_app.intensity_directional)\n",
    "\n",
    "    render_app.light_pos = (light_x, -2, 5)\n",
    "    render_app.intensity_ambient = ambient_w\n",
    "    render_app.intensity_directional = direct_w\n",
    "\n",
    "    facecover_render = render_app(\n",
    "        _to_ctype(face_mesh['ver'].T), face_mesh['tri'], background * 0, face_mesh['colors'].copy(),\n",
    "    )\n",
    "    \n",
    "    render_app.intensity_ambient = 1.0\n",
    "    render_app.intensity_directional = 0.0\n",
    "\n",
    "    alpha_render = render_app(\n",
    "        _to_ctype(face_mesh['ver'].T), face_mesh['tri'], background * 0,\n",
    "        np.stack([face_mesh['alphas']] * 3, axis=-1)\n",
    "    ).astype(np.float32) / 255.0\n",
    "    \n",
    "    prefinal_render = background * (1.0 - alpha_render) + facecover_render * alpha_render\n",
    "\n",
    "    render_app.light_pos = original_light_pos\n",
    "    render_app.intensity_ambient, render_app.intensity_directional = original_light_intensity\n",
    "    \n",
    "    # Light Wrap technique\n",
    "    # https://www.imaging-resource.com/news/2016/02/11/create-natural-looking-composite-images-using-light-wrapping-technique\n",
    "    alpha_rough = np.where(alpha_render[..., 0] > 0.25, 1.0, 0.0)\n",
    "    alpha_rough_border = (np.maximum(*np.abs(np.gradient(alpha_rough))) > 0.0).astype(np.float32)\n",
    "    \n",
    "    alpha_light_wrap = gaussian_filter(alpha_rough_border, sigma=1.25, mode='nearest')[..., None] * 0.5\n",
    "    background_blurred = blur_multichannel(background, sigma=2.5, mode='nearest')\n",
    "\n",
    "    final_render = (\n",
    "        prefinal_render * (1.0 - alpha_light_wrap) + background_blurred * alpha_light_wrap\n",
    "    ).astype(np.uint8)\n",
    "    \n",
    "    if return_intermediate:\n",
    "        return facecover_render, alpha_render, prefinal_render.astype(np.uint8), final_render\n",
    "    return final_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
